# Unified Workflow Configuration
# Single YAML file for research topic, agents, workflow settings, and output config

# Research Topic Configuration
topic:
  topic: "Financial Trading System Integration with LLM-Based Market Analysis Agents"
  keywords: ["financial trading", "LLM", "large language models", "market analysis", "trading systems", "algorithmic trading", "AI agents", "financial AI", "sentiment analysis", "market prediction", "trading automation", "NLP in finance", "GPT in trading", "neural networks trading"]
  domain: "financial technology and algorithmic trading"
  scope: "Focus on integration methodologies for incorporating LLM-based market analysis agents into financial trading systems, including real-time market data processing, sentiment analysis, trading signal generation, and automated execution strategies"
  research_question: "How do LLM-based market analysis agents improve trading system performance, and what integration architectures enable effective real-time market analysis and automated trading decision-making?"
  context: "Large Language Models (LLMs) have shown promise in financial market analysis through natural language processing of news, social media, and financial reports. The integration of LLM-based agents into trading systems represents an emerging area combining NLP, financial engineering, and automated trading. This research examines architectures, performance metrics, and implementation challenges for LLM-enhanced trading systems."

# Agent Configurations
# Model selection guide based on pricing research (2025):
#   Gemini 2.5 Flash-Lite ($0.10/1M input, $0.40/1M output): Bulk screening, high-volume classification - RECOMMENDED for full-text screening
#   Gemini 2.5 Flash ($0.30/1M input, $2.50/1M output): Balanced speed/cost - RECOMMENDED for search and moderate-volume tasks
#   Gemini 2.5 Pro ($1.25/1M input, $10.00/1M output): Quality-critical decisions - RECOMMENDED for borderline screening, extraction, writing
#   Gemini 3 Pro ($2.00/1M input, $12.00/1M output): Most advanced reasoning - OPTIONAL upgrade for Pro tasks
#   Perplexity: sonar-pro, sonar-reasoning-pro, sonar
agents:
  search_agent:
    role: "Literature Search Specialist"
    goal: "Find comprehensive literature on {topic} across multiple databases"
    backstory: "Expert researcher specializing in {domain} with deep knowledge of {topic}"
    # Flash: Optimized for speed, efficiency, and scale. Best price-performance for search tasks.
    # Handles large-scale summarization, responsive chat, efficient data extraction.
    llm_model: "gemini-2.5-flash"
    tools: ["database_search", "query_builder", "exa_search", "tavily_search"]
    temperature: 0.1
    max_iterations: 5
    
  # Title/Abstract Screening - uses Flash-Lite for maximum cost efficiency
  title_abstract_screener:
    role: "Title/Abstract Screening Specialist"
    goal: "Screen papers based on title and abstract for relevance to {topic} and {research_question}"
    backstory: "Expert reviewer with expertise in {domain}, focusing on {topic}. Handles borderline cases requiring careful judgment."
    # Flash-Lite: Best cost efficiency for classification tasks. 3x cheaper than Flash ($0.10 vs $0.30 input, $0.40 vs $2.50 output).
    # 12.5x cheaper than Pro ($0.10 vs $1.25 input, $0.40 vs $10.00 output).
    # Perfect for title/abstract screening where keyword pre-filtering handles most cases, but LLM is needed for uncertain papers.
    # Flash-Lite provides sufficient accuracy for classification tasks while minimizing costs.
    llm_model: "gemini-2.5-flash-lite"
    tools: ["title_screener"]
    temperature: 0.2
    max_iterations: 10
    
  # Full-text Screening - use Flash-Lite for bulk processing (89+ papers)
  fulltext_screener:
    role: "Full-text Screening Specialist"
    goal: "Screen papers based on full-text content for relevance to {topic} and {research_question}"
    backstory: "Efficient reviewer specializing in {topic}. Optimized for high-volume screening tasks."
    # Flash-Lite: Optimized for bulk screening tasks. 4x cheaper than Pro ($0.10 vs $1.25 per 1M tokens).
    # Suitable for high-volume classification where speed and cost matter more than nuanced reasoning.
    # Model selection guide:
    # - Flash-Lite ($0.10): Bulk screening, high-volume tasks (RECOMMENDED for full-text)
    # - Flash ($0.30): Balanced speed/cost for moderate volume
    # - Pro ($1.25): Quality-critical, low-volume decisions
    llm_model: "gemini-2.5-flash-lite"
    tools: ["fulltext_screener"]
    temperature: 0.2
    max_iterations: 5
    
  # DEPRECATED: Use title_abstract_screener and fulltext_screener instead
  # Kept for backward compatibility - will be used as fallback if new configs are missing
  screening_agent:
    role: "Screening Specialist"
    goal: "Screen papers for relevance to {topic} and {research_question}"
    backstory: "Meticulous reviewer with expertise in {domain}, focusing on {topic}"
    # Pro: Advanced reasoning for complex multi-step logic. Higher accuracy for precision-critical tasks.
    # Excels at complex reasoning, advanced coding, large data analysis with fewer errors.
    llm_model: "gemini-2.5-pro"
    tools: ["title_screener", "fulltext_screener"]
    temperature: 0.2
    max_iterations: 10
    
  extraction_agent:
    role: "Data Extraction Specialist"
    goal: "Extract structured data from papers related to {topic}"
    backstory: "Detail-oriented analyst specializing in {domain} research on {topic}"
    # Pro: Handles complex extraction with 90%+ accuracy. Better at structured output and complex reasoning.
    # Delivers more precise answers for technical tasks requiring structured data extraction.
    llm_model: "gemini-2.5-pro"
    tools: ["data_extractor"]
    temperature: 0.1
    max_iterations: 5
    
  introduction_writer:
    role: "Introduction Writer"
    goal: "Write compelling introduction for systematic review on {topic}"
    backstory: "Skilled academic writer with expertise in {domain}, specializing in {topic}"
    # Pro: Better for professional reports, strategic documents, analytical workflows.
    # Delivers more detailed explanations and polished outputs for academic writing.
    llm_model: "gemini-2.5-pro"
    tools: []
    temperature: 0.2
    max_iterations: 3
    
  methods_writer:
    role: "Methods Writer"
    goal: "Write detailed methods section for {topic} systematic review"
    backstory: "Methodology expert in {domain} research, focusing on {topic}"
    # Pro: Better for technical writing, code review, and structured documentation.
    # Handles complex logic and structured output better than Flash.
    llm_model: "gemini-2.5-pro"
    # Tools are registered programmatically: generate_inclusion_exclusion_table, generate_mermaid_diagram
    tools: []
    temperature: 0.2
    max_iterations: 3
    
  results_writer:
    role: "Results Writer"
    goal: "Synthesize and present results for {topic} systematic review"
    backstory: "Data synthesis expert in {domain}, specializing in {topic}"
    # Pro: Better for synthesizing large datasets, making sense of complex information.
    # Excels at data analysis and presenting insights from dense information.
    llm_model: "gemini-2.5-pro"
    # Tools are registered programmatically: generate_mermaid_diagram, generate_thematic_table, generate_topic_analysis_table
    tools: []
    temperature: 0.2
    max_iterations: 3
    
  discussion_writer:
    role: "Discussion Writer"
    goal: "Write comprehensive discussion for {topic} systematic review"
    backstory: "Critical analysis expert in {domain}, focusing on {topic}"
    # Pro: Best for critical analysis, complex reasoning, and nuanced interpretation.
    # Handles multi-step logic and provides deeper insights for discussion sections.
    llm_model: "gemini-2.5-pro"
    tools: []
    temperature: 0.2
    max_iterations: 3
    
  abstract_generator:
    role: "Abstract Generator"
    goal: "Generate concise abstract summarizing {topic} systematic review"
    backstory: "Expert academic writer specializing in {domain}, skilled at creating compelling abstracts"
    # Flash: Sufficient quality for abstract generation. Abstracts are shorter and less complex than full sections.
    # 60-75% cheaper than Pro ($0.30 vs $1.25 input, $2.50 vs $10.00 output).
    # Flash provides good quality for abstract generation while maintaining cost efficiency.
    llm_model: "gemini-2.5-flash"
    tools: []
    temperature: 0.2
    max_iterations: 3

# Workflow Settings
workflow:
  databases: ["PubMed", "arXiv", "Semantic Scholar", "Crossref", "ACM", "Springer", "IEEE Xplore"]
  date_range:
    start: null  # null means no start limit
    end: 2025
  language: "English"
  max_results_per_db: 100
  similarity_threshold: 85  # For deduplication
  
  # Database-specific settings
  database_settings:
    PubMed:
      enabled: true
      max_results: 100
      rate_limit: 3  # requests per second
      requires_api_key: false  # Optional but recommended
    arXiv:
      enabled: true
      max_results: 100
      rate_limit: 3  # requests per second
      requires_api_key: false
    "Semantic Scholar":
      enabled: true
      max_results: 100
      rate_limit: 1  # requests per second (with API key)
      requires_api_key: false  # Optional but recommended for higher limits
    Crossref:
      enabled: true
      max_results: 100
      rate_limit: 10  # requests per second (conservative)
      requires_api_key: false
      requires_email: true  # Recommended
    Scopus:
      enabled: false  # Requires API key
      max_results: 100
      rate_limit: 9  # requests per second
      requires_api_key: true
    "Web of Science":
      enabled: false  # Requires API key
      max_results: 100
      rate_limit: 5
      requires_api_key: true
    "IEEE Xplore":
      enabled: true  # Web scraping, no API key needed
      max_results: 100
      rate_limit: 2  # Conservative rate limit for web scraping
      requires_api_key: false
    ACM:
      enabled: true
      max_results: 100
      rate_limit: 2  # Conservative rate limit for web scraping
      requires_api_key: false
    Springer:
      enabled: true
      max_results: 100
      rate_limit: 2  # Conservative rate limit for web scraping
      requires_api_key: false
    "Google Scholar":
      enabled: false  # Requires proxy for reliable operation
      max_results: 100
      rate_limit: 1  # Very conservative - Google Scholar has strict rate limits
      requires_api_key: false
      requires_proxy: true  # Highly recommended to avoid CAPTCHAs
  
  # Bibliometrics settings (enhanced features from pybliometrics and scholarly)
  bibliometrics:
    enabled: false  # Enable bibliometric features (requires pybliometrics/scholarly)
    include_author_metrics: true  # Include h-index, citation counts, etc.
    include_citation_networks: true  # Build citation networks
    include_subject_areas: true  # Include subject area classifications
    include_coauthors: true  # Include coauthor information
    include_affiliations: true  # Include detailed affiliation data
  
  # Google Scholar specific settings
  google_scholar:
    enabled: false  # Enable Google Scholar connector
    use_proxy: true  # Required for reliable operation (avoids CAPTCHAs)
    proxy_type: "scraperapi"  # Options: scraperapi, free, none
    max_author_results: 10  # Max authors to retrieve per search
    max_citation_results: 100  # Max citing papers to retrieve
  
  # Caching settings
  cache:
    enabled: true
    ttl_hours: 24  # Time-to-live for cached results
    cache_dir: "data/cache"
  
  # Proxy settings
  proxy:
    enabled: false
    type: "none"  # Options: none, http, socks5, scraperapi, free
    http_proxy: "${HTTP_PROXY}"  # HTTP proxy URL (e.g., "http://proxy.example.com:8080")
    https_proxy: "${HTTPS_PROXY}"  # HTTPS proxy URL (defaults to http_proxy if not specified)
    scraperapi_key: "${SCRAPERAPI_KEY}"  # ScraperAPI key (required for scraperapi type)
    rotation_on_failure: true  # Rotate proxy on failure
    timeout: 5.0  # Timeout for proxy health checks (seconds)
  
  # Integrity checking settings
  integrity:
    enabled: true
    required_fields: ["title", "authors"]  # Fields that must be present
    action: "warn"  # Options: warn (log warning) or raise (raise exception)
  
  # Session management settings
  session:
    persistent: true  # Use persistent HTTP sessions with cookie handling
    cookie_jar: "data/cookies"  # Directory for storing cookies
  
  # Search logging (PRISMA compliance)
  search_logging:
    enabled: true
    log_dir: "data/outputs/search_logs"
    generate_prisma_report: true
    generate_csv_summary: true
  
# Inclusion/Exclusion Criteria
criteria:
  inclusion:
    - "Studies on LLM-based, AI agent, or machine learning systems for financial market analysis"
    - "Integration of language models (GPT, BERT, etc.) or AI/ML models into trading systems"
    - "Market analysis using NLP, sentiment analysis, text-based data, or AI/ML techniques"
    - "Automated trading systems with AI/ML/NLP components"
    - "Sentiment analysis or text mining applied to financial markets and trading"
    - "Evaluation of trading performance, accuracy, or profitability metrics"
    - "Published in English and peer-reviewed journals or conferences"
  exclusion:
    - "Studies without any AI/ML/NLP components (pure traditional statistical or rule-based trading strategies)"
    - "Non-financial market applications (e.g., cryptocurrency-only without traditional markets)"
    - "Pure theoretical papers without implementation or empirical evaluation"
    - "Studies focused only on backtesting without real-world integration or system architecture"
    - "Opinion pieces or non-peer-reviewed sources"
    - "Conference abstracts without full papers"

# Screening Safeguards
screening_safeguards:
  minimum_papers: 5  # Minimum papers required to pass full-text screening (lowered to allow workflow to proceed)
  enable_manual_review: true  # Pause workflow for manual review if threshold not met
  show_borderline_papers: true  # Show borderline papers for review

# Search Terms (will be merged with topic keywords if provided)
search_terms:
  llm_trading: ["LLM trading", "GPT trading", "language model trading", "transformer trading", "BERT finance", "NLP trading", "large language model finance"]
  market_analysis: ["market analysis", "financial analysis", "market prediction", "price prediction", "trading signals", "market forecasting", "financial forecasting"]
  trading_systems: ["trading systems", "algorithmic trading", "automated trading", "trading platforms", "execution systems", "trading infrastructure", "trading architecture"]
  sentiment_analysis: ["sentiment analysis", "financial sentiment", "news sentiment", "market sentiment", "social media sentiment", "text sentiment finance"]
  ai_agents: ["AI agents", "trading agents", "autonomous agents", "intelligent agents", "agent-based trading", "multi-agent systems"]
  integration: ["system integration", "trading integration", "API integration", "real-time integration", "market data integration", "trading platform integration"]

# Protocol Registration
protocol:
  registered: false  # Set to true if protocol is registered
  registry: "PROSPERO"  # Options: "PROSPERO", "OSF", "Other"
  registration_number: ""  # Fill in if registered (e.g., "CRD42025XXXXXX")
  url: ""  # Fill in if registered (e.g., "https://www.crd.york.ac.uk/prospero/display_record.php?ID=CRD42025XXXXXX")

# Supplementary Materials
supplementary_materials:
  search_strategies: true  # Include full search strategies
  extracted_data: true  # Include extracted data
  analysis_code: false  # Include analysis code if available
  other_materials: []  # List any other supplementary materials

# Writing Configuration
writing:
  style_extraction:
    enabled: true                    # Enable/disable style pattern extraction
    model: "gemini-2.5-pro"          # LLM model for section extraction
    max_papers: null                 # Max papers to analyze (null = all eligible)
    min_papers: 3                    # Minimum papers required for pattern extraction
  
  humanization:
    enabled: true                    # Enable/disable humanization
    model: "gemini-2.5-pro"          # LLM model for humanization
    temperature: 0.3                  # Temperature for variation
    max_iterations: 2                # Max refinement iterations
    naturalness_threshold: 0.75      # Minimum naturalness score
    section_specific: true            # Use section-specific strategies

# Quality Assessment Configuration
quality_assessment:
  risk_of_bias_tool: "RoB 2"  # Options: "RoB 2", "ROBINS-I", "CASP"
  grade_assessment: true  # Enable GRADE certainty assessment
  auto_fill: true  # Automatically fill assessments using LLM (default: true, set to false for manual assessment)
  template_path: "data/quality_assessments/{workflow_id}_assessments.json"  # Path template for assessment file

# Funding Information
funding:
  source: "No funding received"  # Update with actual funding source if applicable
  grant_number: ""  # Grant number if applicable
  funder: ""  # Funder name if applicable

# Conflicts of Interest
conflicts_of_interest:
  statement: "The authors declare no conflicts of interest."  # Update as needed

# Output Configuration
output:
  directory: "data/outputs"
  formats: ["markdown", "json", "bibtex", "ris"]  # ris: Research Information Systems format
  generate_prisma: true
  generate_charts: true

# Manubot Configuration
manubot:
  enabled: true  # Set to true to enable Manubot export
  output_dir: "manuscript"  # Relative to data/outputs/
  citation_style: "ieee"  # Options: "ieee", "apa", "nature", "plos", etc.
  auto_resolve_citations: true  # Automatically resolve citations from DOI/PMID
  generate_manubot_yaml: true  # Generate manubot.yaml configuration file

# Submission Package Configuration
submission:
  enabled: true  # Set to true to enable submission package generation
  default_journal: "ieee"  # Default journal for submission package
  generate_pdf: true  # Generate PDF manuscript
  generate_docx: true  # Generate Word document
  generate_html: true  # Generate HTML version
  include_supplementary: true  # Include supplementary materials
  validate_before_package: true  # Validate submission before packaging
