# Unified Workflow Configuration
# Single YAML file for research topic, agents, workflow settings, and output config

# Research Topic Configuration
# Everything you need to configure for a new study is in this section
topic:
  topic: "Conversational AI Tutors for Health Science Education"
  keywords: ["conversational AI tutor", "AI chatbot education", "virtual teaching assistant", "dialogue-based tutoring", "conversational agent health education", "AI medical education", "chatbot nursing education", "intelligent tutoring system health", "conversational learning health sciences", "AI-powered tutor healthcare"]
  domain: "AI-powered educational technology in health sciences"
  scope: "Focus on conversational AI tutors (chatbots, virtual teaching assistants, dialogue-based tutoring systems) used in health science education (medicine, nursing, pharmacy, public health, allied health), including learning outcomes, student engagement, pedagogical approaches, and implementation strategies"
  research_question: "How do conversational AI tutors impact learning outcomes, engagement, and knowledge retention in health science education, and what are the key design factors and implementation challenges?"
  context: "Conversational AI tutors are increasingly being adopted in health science education to provide personalized, on-demand learning support. These systems can supplement traditional instruction, provide clinical scenario practice, and help students master complex medical concepts through interactive dialogue. This review examines the effectiveness, pedagogical approaches, and implementation considerations for conversational AI tutoring systems in health science education."
  
  # Inclusion/Exclusion Criteria
  inclusion:
    - "Studies on conversational AI tutors or chatbots for health science education"
    - "Implementation in medical, nursing, pharmacy, or allied health programs"
    - "Learning outcomes, knowledge retention, or skill development measures"
    - "Student engagement, satisfaction, or user experience evaluations"
    - "Pedagogical design, conversational strategies, or AI techniques"
    - "Published in English and peer-reviewed"
  exclusion:
    - "Non-conversational educational technology (static content, videos only, non-interactive)"
    - "Non-health science domains (general education, business, engineering, etc.)"
    - "Studies without empirical evaluation or outcomes data"
    - "Opinion pieces or non-peer-reviewed sources"
    - "Conference abstracts without full papers"
  
  # Protocol Registration (PRISMA 2020)
  protocol:
    registered: false  # Set to true if protocol is registered
    registry: "PROSPERO"  # Options: "PROSPERO", "OSF", "Other"
    registration_number: ""  # Fill in if registered (e.g., "CRD42025XXXXXX")
    url: ""  # Fill in if registered (e.g., "https://www.crd.york.ac.uk/prospero/display_record.php?ID=CRD42025XXXXXX")
  
  # Funding Information
  funding:
    source: "No funding received"  # Update with actual funding source if applicable
    grant_number: ""  # Grant number if applicable
    funder: ""  # Funder name if applicable
  
  # Conflicts of Interest
  conflicts_of_interest:
    statement: "The authors declare no conflicts of interest."  # Update as needed

# Agent Configurations
# LLM Provider: Google Gemini (gemini-2.5-* models) is the primary and recommended provider
# Model selection guide based on pricing research (2026):
#   Gemini 2.5 Flash-Lite ($0.10/1M input, $0.40/1M output): Bulk screening, high-volume classification - RECOMMENDED for full-text screening
#   Gemini 2.5 Flash ($0.30/1M input, $2.50/1M output): Balanced speed/cost - RECOMMENDED for search and moderate-volume tasks
#   Gemini 2.5 Pro ($1.25/1M input, $10.00/1M output): Quality-critical decisions - RECOMMENDED for borderline screening, extraction, writing
# Note: Perplexity is supported for academic SEARCH (via PERPLEXITY_SEARCH_API_KEY), not for LLM tasks
agents:
  search_agent:
    role: "Literature Search Specialist"
    goal: "Find comprehensive literature on {topic} across multiple databases"
    backstory: "Expert researcher specializing in {domain} with deep knowledge of {topic}"
    # Flash: Optimized for speed, efficiency, and scale. Best price-performance for search tasks.
    # Handles large-scale summarization, responsive chat, efficient data extraction.
    llm_model: "gemini-2.5-flash"
    tools: ["database_search", "query_builder", "exa_search", "tavily_search"]
    temperature: 0.1
    max_iterations: 5
    
  # Title/Abstract Screening - uses Flash-Lite for maximum cost efficiency
  title_abstract_screener:
    role: "Title/Abstract Screening Specialist"
    goal: "Screen papers based on title and abstract for relevance to {topic} and {research_question}"
    backstory: "Expert reviewer with expertise in {domain}, focusing on {topic}. Handles borderline cases requiring careful judgment."
    # Flash-Lite: Best cost efficiency for classification tasks. 3x cheaper than Flash ($0.10 vs $0.30 input, $0.40 vs $2.50 output).
    # 12.5x cheaper than Pro ($0.10 vs $1.25 input, $0.40 vs $10.00 output).
    # Perfect for title/abstract screening where keyword pre-filtering handles most cases, but LLM is needed for uncertain papers.
    # Flash-Lite provides sufficient accuracy for classification tasks while minimizing costs.
    llm_model: "gemini-2.5-flash-lite"
    tools: ["title_screener"]
    temperature: 0.2
    max_iterations: 10
    
  # Full-text Screening - use Flash-Lite for bulk processing (89+ papers)
  fulltext_screener:
    role: "Full-text Screening Specialist"
    goal: "Screen papers based on full-text content for relevance to {topic} and {research_question}"
    backstory: "Efficient reviewer specializing in {topic}. Optimized for high-volume screening tasks."
    # Flash-Lite: Optimized for bulk screening tasks. 4x cheaper than Pro ($0.10 vs $1.25 per 1M tokens).
    # Suitable for high-volume classification where speed and cost matter more than nuanced reasoning.
    # Model selection guide:
    # - Flash-Lite ($0.10): Bulk screening, high-volume tasks (RECOMMENDED for full-text)
    # - Flash ($0.30): Balanced speed/cost for moderate volume
    # - Pro ($1.25): Quality-critical, low-volume decisions
    llm_model: "gemini-2.5-flash-lite"
    tools: ["fulltext_screener"]
    temperature: 0.2
    max_iterations: 5
    
  extraction_agent:
    role: "Data Extraction Specialist"
    goal: "Extract structured data from papers related to {topic}"
    backstory: "Detail-oriented analyst specializing in {domain} research on {topic}"
    # Pro: Handles complex extraction with 90%+ accuracy. Better at structured output and complex reasoning.
    # Delivers more precise answers for technical tasks requiring structured data extraction.
    llm_model: "gemini-2.5-pro"
    tools: ["data_extractor"]
    temperature: 0.1
    max_iterations: 5
    
  introduction_writer:
    role: "Introduction Writer"
    goal: "Write compelling introduction for systematic review on {topic}"
    backstory: "Skilled academic writer with expertise in {domain}, specializing in {topic}"
    # Pro: Better for professional reports, strategic documents, analytical workflows.
    # Delivers more detailed explanations and polished outputs for academic writing.
    llm_model: "gemini-2.5-pro"
    tools: []
    temperature: 0.2
    max_iterations: 3
    
  methods_writer:
    role: "Methods Writer"
    goal: "Write detailed methods section for {topic} systematic review"
    backstory: "Methodology expert in {domain} research, focusing on {topic}"
    # Pro: Better for technical writing, code review, and structured documentation.
    # Handles complex logic and structured output better than Flash.
    llm_model: "gemini-2.5-pro"
    # Tools are registered programmatically: generate_inclusion_exclusion_table, generate_mermaid_diagram
    tools: []
    temperature: 0.2
    max_iterations: 3
    
  results_writer:
    role: "Results Writer"
    goal: "Synthesize and present results for {topic} systematic review"
    backstory: "Data synthesis expert in {domain}, specializing in {topic}"
    # Pro: Better for synthesizing large datasets, making sense of complex information.
    # Excels at data analysis and presenting insights from dense information.
    llm_model: "gemini-2.5-pro"
    # Tools are registered programmatically: generate_mermaid_diagram, generate_thematic_table, generate_topic_analysis_table
    tools: []
    temperature: 0.2
    max_iterations: 3
    
  discussion_writer:
    role: "Discussion Writer"
    goal: "Write comprehensive discussion for {topic} systematic review"
    backstory: "Critical analysis expert in {domain}, focusing on {topic}"
    # Pro: Best for critical analysis, complex reasoning, and nuanced interpretation.
    # Handles multi-step logic and provides deeper insights for discussion sections.
    llm_model: "gemini-2.5-pro"
    tools: []
    temperature: 0.2
    max_iterations: 3
    
  abstract_generator:
    role: "Abstract Generator"
    goal: "Generate concise abstract summarizing {topic} systematic review"
    backstory: "Expert academic writer specializing in {domain}, skilled at creating compelling abstracts"
    # Flash: Sufficient quality for abstract generation. Abstracts are shorter and less complex than full sections.
    # 60-75% cheaper than Pro ($0.30 vs $1.25 input, $2.50 vs $10.00 output).
    # Flash provides good quality for abstract generation while maintaining cost efficiency.
    llm_model: "gemini-2.5-flash"
    tools: []
    temperature: 0.2
    max_iterations: 3

# Workflow Settings
workflow:
  databases: ["PubMed", "arXiv", "Semantic Scholar", "Crossref", "Scopus", "ACM", "Springer", "IEEE Xplore", "Perplexity"]
  date_range:
    start: 2015  # Last 10 years - includes early chatbot research
    end: 2025
  language: "English"
  max_results_per_db: 100
  similarity_threshold: 85  # For deduplication
  
  # Database-specific settings
  database_settings:
    PubMed:
      enabled: true
      max_results: 100
      rate_limit: 3  # requests per second
      requires_api_key: false  # Optional but recommended
    arXiv:
      enabled: true
      max_results: 100
      rate_limit: 3  # requests per second
      requires_api_key: false
    "Semantic Scholar":
      enabled: true
      max_results: 100
      rate_limit: 1  # requests per second (with API key)
      requires_api_key: false  # Optional but recommended for higher limits
    Crossref:
      enabled: true
      max_results: 100
      rate_limit: 10  # requests per second (conservative)
      requires_api_key: false
      requires_email: true  # Recommended
    Scopus:
      enabled: true  # Requires API key
      max_results: 100
      rate_limit: 9  # requests per second
      requires_api_key: true
    "Web of Science":
      enabled: false  # Requires API key
      max_results: 100
      rate_limit: 5
      requires_api_key: true
    "IEEE Xplore":
      enabled: true  # Web scraping, no API key needed
      max_results: 100
      rate_limit: 2  # Conservative rate limit for web scraping
      requires_api_key: false
    ACM:
      enabled: true
      max_results: 100
      rate_limit: 2  # Conservative rate limit for web scraping
      requires_api_key: false
    Springer:
      enabled: true
      max_results: 100
      rate_limit: 2  # Conservative rate limit for web scraping
      requires_api_key: false
    "Google Scholar":
      enabled: true  # Requires proxy for reliable operation
      max_results: 100
      rate_limit: 1  # Very conservative - Google Scholar has strict rate limits
      requires_api_key: false
      requires_proxy: true  # Highly recommended to avoid CAPTCHAs
    Perplexity:
      enabled: true  # Requires PERPLEXITY_SEARCH_API_KEY (separate from LLM API key)
      max_results: 100
      rate_limit: 5  # requests per second (conservative)
      requires_api_key: true
      # Domain filtering mode: "allowlist" (include only academic domains, may miss some),
      # "denylist" (exclude non-academic domains, captures all academic sources),
      # or "none" (no filtering). Default: "allowlist"
      # Set via PERPLEXITY_DOMAIN_FILTER_MODE environment variable
      domain_filter_mode: "allowlist"  # Options: allowlist, denylist, none
  
  # Bibliometrics settings (enhanced features from pybliometrics and scholarly)
  bibliometrics:
    enabled: true  # Enable bibliometric features (requires pybliometrics/scholarly)
    include_author_metrics: true  # Include h-index, citation counts, etc.
    include_citation_networks: false  # Build citation networks (disabled for basic setup)
    include_subject_areas: true  # Include subject area classifications
    include_coauthors: false  # Include coauthor information (disabled for basic setup)
    include_affiliations: false  # Include detailed affiliation data (disabled for basic setup)
  
  # Google Scholar specific settings
  google_scholar:
    enabled: false  # Enable Google Scholar connector
    use_proxy: true  # Required for reliable operation (avoids CAPTCHAs)
    proxy_type: "scraperapi"  # Options: scraperapi, free, none
    max_author_results: 10  # Max authors to retrieve per search
    max_citation_results: 100  # Max citing papers to retrieve
  
  # Caching settings
  cache:
    enabled: true
    ttl_hours: 24  # Time-to-live for cached results
    cache_dir: "data/cache"
  
  # Proxy settings
  proxy:
    enabled: false
    type: "none"  # Options: none, http, socks5, scraperapi, free
    http_proxy: "${HTTP_PROXY}"  # HTTP proxy URL (e.g., "http://proxy.example.com:8080")
    https_proxy: "${HTTPS_PROXY}"  # HTTPS proxy URL (defaults to http_proxy if not specified)
    scraperapi_key: "${SCRAPERAPI_KEY}"  # ScraperAPI key (required for scraperapi type)
    rotation_on_failure: true  # Rotate proxy on failure
    timeout: 5.0  # Timeout for proxy health checks (seconds)
  
  # Integrity checking settings
  integrity:
    enabled: true
    required_fields: ["title", "authors"]  # Fields that must be present
    action: "warn"  # Options: warn (log warning) or raise (raise exception)
  
  # Session management settings
  session:
    persistent: true  # Use persistent HTTP sessions with cookie handling
    cookie_jar: "data/cookies"  # Directory for storing cookies
  
  # Search logging (PRISMA compliance)
  search_logging:
    enabled: true
    log_dir: "data/outputs/search_logs"
    generate_prisma_report: true
    generate_csv_summary: true
  
  # Parallel execution controls for late workflow phases
  parallel_execution:
    deconflict_llm_heavy_phases: true  # run quality_assessment and article_writing separately
  
# Screening Safeguards
screening_safeguards:
  minimum_papers: 5  # Minimum papers required to pass full-text screening (lowered to allow workflow to proceed)
  enable_manual_review: true  # Pause workflow for manual review if threshold not met
  show_borderline_papers: true  # Show borderline papers for review
  stage1_include_threshold: 0.85  # stricter auto-include threshold for keyword prefilter
  stage1_exclude_threshold: 0.8

# Extraction quality controls
extraction_quality:
  max_empty_rate: 0.35  # gate if >35% papers have empty core extraction fields
  enforce_gate: true

# Supplementary Materials (study-specific, but kept separate as it's more of a workflow setting)
supplementary_materials:
  search_strategies: true  # Include full search strategies
  extracted_data: true  # Include extracted data
  analysis_code: false  # Include analysis code if available
  other_materials: []  # List any other supplementary materials

# Writing Configuration
writing:
  style_extraction:
    enabled: true                    # Enable/disable style pattern extraction
    model: "gemini-2.5-pro"          # LLM model for section extraction
    max_papers: null                 # Max papers to analyze (null = all eligible)
    min_papers: 3                    # Minimum papers required for pattern extraction
    timeout: 120                     # Timeout for LLM calls in seconds (default: 120)
  
  humanization:
    enabled: true                    # Enable/disable humanization
    model: "gemini-2.5-pro"          # LLM model for humanization
    temperature: 0.3                  # Temperature for variation
    max_iterations: 2                # Max refinement iterations
    naturalness_threshold: 0.75      # Minimum naturalness score
    section_specific: true            # Use section-specific strategies
  
  retry_count: 1                     # Number of retry attempts for section writing (reduced to 1 to avoid long waits)
  checkpoint_per_section: true       # Save checkpoint after each section completes (enables resume from last completed section)
  llm_timeout: 120                   # Timeout for individual LLM calls in seconds (prevents hanging)

# Quality Assessment Configuration
quality_assessment:
  # Assessment framework
  framework: "CASP"  # Critical Appraisal Skills Programme
  
  # CASP settings
  casp:
    enabled_checklists:
      - rct          # CASP RCT Checklist (11 questions)
      - cohort       # CASP Cohort Study Checklist (12 questions)
      - qualitative  # CASP Qualitative Research Checklist (10 questions)
    
    # Study type detection
    auto_detect:
      enabled: true
      method: "llm"  # Use LLM to analyze study and select checklist
      llm_model: "gemini-2.5-flash"  # Fast classification
      confidence_threshold: 0.7  # Use cohort as fallback if confidence lower
      fallback_checklist: "cohort"  # Most flexible for uncertain cases
    
    # Manual override (optional)
    allow_manual_override: true  # Users can specify checklist in template if needed
  
  # GRADE assessment (complementary to CASP)
  grade_assessment: true  # Enable GRADE certainty assessment
  grade_outcomes: []  # Auto-detected from extracted outcomes
  
  # LLM settings for quality assessment
  assessment_llm:
    model: "gemini-2.5-pro"  # Pro model for quality-critical assessment
    temperature: 0.2
    timeout: 120
  
  # Auto-fill settings
  auto_fill: true  # Automatically fill using LLM
  
  # Output paths
  template_path: "data/quality_assessments/{workflow_id}_assessments.json"


# Output Configuration
output:
  directory: "data/outputs"
  formats: ["markdown", "json", "bibtex", "ris"]  # ris: Research Information Systems format
  generate_prisma: true
  generate_charts: true

# Manubot Configuration
manubot:
  enabled: false  # Set to true to enable Manubot export (disabled by default - use submission package instead)
  output_dir: "manuscript"  # Relative to data/outputs/
  citation_style: "ieee"  # Options: "ieee", "apa", "nature", "plos", etc.
  auto_resolve_citations: true  # Automatically resolve citations from DOI/PMID
  generate_manubot_yaml: true  # Generate manubot.yaml configuration file

# Submission Package Configuration
submission:
  enabled: true  # Set to true to enable submission package generation
  default_journal: "ieee"  # Default journal for submission package
  generate_pdf: true  # Generate PDF manuscript
  generate_docx: true  # Generate Word document
  generate_html: true  # Generate HTML version
  include_supplementary: true  # Include supplementary materials
  validate_before_package: true  # Validate submission before packaging
