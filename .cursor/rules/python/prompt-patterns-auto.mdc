---
description:
globs: src/screening/**/*.py, src/extraction/**/*.py, src/quality/**/*.py, src/writing/**/*.py
alwaysApply: false
---

# LLM Prompt Engineering Patterns

## Topic Context Injection (every prompt header)
Every LLM prompt starts with this context block, populated from ReviewConfig:
```
Role: {role from settings.yaml}
Goal: {goal with topic and research_question interpolated}
Backstory: {backstory with domain interpolated}
Topic: {topic}
Research Question: {research_question}
Domain: {domain}
Keywords: {keywords joined by comma}
```

## Structured Output Enforcement
All prompts end with:
"Return ONLY valid JSON matching this exact schema: {schema}"

## Truncation Limits (context window + cost control)
- Title/abstract screening: full title + abstract (no truncation)
- Full-text screening: first 8,000 characters of full text
- Data extraction: first 10,000 characters of full text

## Confidence Thresholds (from config/settings.yaml)
- Auto-include if confidence >= stage1_include_threshold (0.85)
- Auto-exclude if confidence >= stage1_exclude_threshold (0.80)
- Papers between thresholds -> sent to adjudicator
